{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fraud Detection Using Variational AutoEncoders**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TnHItVVXfk39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is the list of our group members:**\n",
        "\n",
        "1. Vadapalli Sai Sravan (CS24MTECH02007)\n",
        "2. Supreet Shukla (CS24MTECH02004)\n",
        "3. Tarun Jangir (CS24MTECH02005)\n",
        "4. Taufique Ramzan Shaikh (CS24MTECH02006)\n",
        "5. Afzaal Ahmad (CS24MTECH02002)"
      ],
      "metadata": {
        "id": "OHctlVpSiokq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the First Variational Auto Encoder**"
      ],
      "metadata": {
        "id": "RkT2PfLVfsgB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjaqN_3fn6EQ",
        "outputId": "57f5698a-8aa7-4555-c4b4-a8277c458169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8876/8876 [==============================] - 23s 2ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 2/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 3/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 4/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 5/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 6/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 7/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 8/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 9/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 10/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 11/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 12/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 13/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 14/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 15/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 16/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 17/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 18/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 19/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 20/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 21/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 22/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 23/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 24/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 25/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 26/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 27/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 28/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 29/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 30/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 31/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 32/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 33/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 34/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 35/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 36/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 37/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 38/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 39/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 40/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 41/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 42/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 43/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 44/100\n",
            "8876/8876 [==============================] - 19s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 45/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 46/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 47/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 48/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 49/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 50/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 51/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 52/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 53/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 54/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 55/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 56/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 57/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 58/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 59/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 60/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 61/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 62/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 63/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 64/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 65/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 66/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 67/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 68/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 69/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 70/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 71/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 72/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 73/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 74/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 75/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 76/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 77/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 78/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 79/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 80/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 81/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 82/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 83/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 84/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 85/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 86/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 87/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 88/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 89/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 90/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 91/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 92/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 93/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 94/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 95/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 96/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 97/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 98/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 99/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 100/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "8876/8876 [==============================] - 15s 2ms/step\n",
            "25/25 [==============================] - 0s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import mse\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load and preprocess data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/creditcard.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']\n",
        "\n",
        "# Log-scale the time difference and amount features\n",
        "time_diff_idx = X.columns.get_loc('Time')\n",
        "amount_idx = X.columns.get_loc('Amount')\n",
        "\n",
        "X.iloc[:, time_diff_idx] = np.log1p(X.iloc[:, time_diff_idx])  # Log-scale time difference\n",
        "X.iloc[:, amount_idx] = np.log1p(X.iloc[:, amount_idx])  # Log-scale amount\n",
        "\n",
        "# Normalize features to the range (0, 1)\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Separate fraud and non-fraud transactions\n",
        "non_fraud_idx = np.where(y == 0)[0]\n",
        "fraud_idx = np.where(y == 1)[0]\n",
        "\n",
        "# Split non-fraud data into train and validation sets\n",
        "X_non_fraud = X[non_fraud_idx]\n",
        "y_non_fraud = y[non_fraud_idx]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_non_fraud, y_non_fraud, test_size=0.001, random_state=42)\n",
        "\n",
        "# Create test set with remaining non-fraud data and all fraud data\n",
        "X_test_non_fraud = np.delete(X[non_fraud_idx], np.concatenate((np.arange(len(X_train)), np.arange(len(X_val)))), axis=0)\n",
        "X_test = np.concatenate((X_test_non_fraud, X[fraud_idx]), axis=0)\n",
        "y_test_non_fraud = np.delete(y[non_fraud_idx], np.concatenate((np.arange(len(y_train)), np.arange(len(y_val)))), axis=0)\n",
        "y_test = np.concatenate((y_test_non_fraud, y[fraud_idx]), axis=0)\n",
        "\n",
        "# Define the variational autoencoder architecture\n",
        "input_dim = X_train.shape[1]\n",
        "latent_dim = 14\n",
        "\n",
        "# Encoder architecture\n",
        "encoder_input = Input(shape=(input_dim,))\n",
        "x = Dense(64, activation='relu')(encoder_input)\n",
        "z_mean = Dense(latent_dim)(x)\n",
        "z_log_var = Dense(latent_dim)(x)\n",
        "\n",
        "# Sampling function\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(tf.keras.backend.shape(z_mean)[0], latent_dim), mean=0., stddev=1.0)\n",
        "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "# Decoder architecture\n",
        "decoder_input = Input(shape=(latent_dim,))\n",
        "x = Dense(64, activation='relu')(decoder_input)\n",
        "decoder_output = Dense(input_dim, activation='sigmoid')(x)\n",
        "\n",
        "# Define the VAE model\n",
        "encoder = Model(encoder_input, [z_mean, z_log_var, z], name='encoder')\n",
        "decoder = Model(decoder_input, decoder_output, name='decoder')\n",
        "vae_output = decoder(encoder(encoder_input)[2])\n",
        "vae = Model(encoder_input, vae_output, name='vae')\n",
        "\n",
        "# Define the VAE loss\n",
        "reconstruction_loss = mse(encoder_input, vae_output)\n",
        "kl_loss = -0.5 * tf.keras.backend.mean(1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var), axis=-1)\n",
        "vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "\n",
        "# Compile the VAE model\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# Train the VAE\n",
        "vae.fit(X_train, X_train, epochs=100, batch_size=32, shuffle=True, validation_data=(X_val, X_val))\n",
        "\n",
        "# Compute reconstruction errors for training data\n",
        "train_reconstructions = vae.predict(X_train)\n",
        "reconstruction_errors_train = np.mean(np.square(X_train - train_reconstructions), axis=1)\n",
        "\n",
        "# Compute the mean reconstruction error on non-fraud training data\n",
        "non_fraud_train_errors = reconstruction_errors_train\n",
        "\n",
        "# Calculate threshold using the reconstruction errors of non-fraudulent transactions in the training set\n",
        "threshold = np.mean(non_fraud_train_errors) + 1.5 * np.std(non_fraud_train_errors)\n",
        "\n",
        "# Compute reconstruction errors for testing data\n",
        "test_reconstructions = vae.predict(X_test)\n",
        "reconstruction_errors_test = np.mean(np.square(X_test - test_reconstructions), axis=1)\n",
        "\n",
        "# Classify transactions as fraud or non-fraud based on the threshold\n",
        "y_pred_test = [1 if e > threshold else 0 for e in reconstruction_errors_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate model performance on test set using first Variational Auto Encoder**"
      ],
      "metadata": {
        "id": "RtpLAMbXf7BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score,  recall_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred_test)\n",
        "print(f\"precision is: {precision}\")\n",
        "recall = recall_score(y_test, y_pred_test)\n",
        "print(f\"recall is: {recall}\")\n",
        "\n",
        "# Evaluate model performance on test set\n",
        "f1 = f1_score(y_test, y_pred_test)\n",
        "print(f'F1 score on test set: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JojAJEhosl4",
        "outputId": "53772031-81dd-47f0-aa50-0962684eb632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision is: 0.9691211401425178\n",
            "recall is: 0.8292682926829268\n",
            "F1 score on test set: 0.8937568455640744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the Second Variational Auto Encoder**"
      ],
      "metadata": {
        "id": "Rt8sFZkPf0Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import mse\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load and preprocess data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/creditcard.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']\n",
        "\n",
        "# Log-scale the time difference and amount features\n",
        "time_diff_idx = X.columns.get_loc('Time')\n",
        "amount_idx = X.columns.get_loc('Amount')\n",
        "\n",
        "X.iloc[:, time_diff_idx] = np.log1p(X.iloc[:, time_diff_idx])  # Log-scale time difference\n",
        "X.iloc[:, amount_idx] = np.log1p(X.iloc[:, amount_idx])  # Log-scale amount\n",
        "\n",
        "# Normalize features to the range (0, 1)\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Separate fraud and non-fraud transactions\n",
        "non_fraud_idx = np.where(y == 0)[0]\n",
        "fraud_idx = np.where(y == 1)[0]\n",
        "\n",
        "# Split non-fraud data into train and validation sets\n",
        "X_non_fraud = X[non_fraud_idx]\n",
        "y_non_fraud = y[non_fraud_idx]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_non_fraud, y_non_fraud, test_size=0.001, random_state=42)\n",
        "\n",
        "# Create test set with remaining non-fraud data and all fraud data\n",
        "X_test_non_fraud = np.delete(X[non_fraud_idx], np.concatenate((np.arange(len(X_train)), np.arange(len(X_val)))), axis=0)\n",
        "X_test = np.concatenate((X_test_non_fraud, X[fraud_idx]), axis=0)\n",
        "y_test_non_fraud = np.delete(y[non_fraud_idx], np.concatenate((np.arange(len(y_train)), np.arange(len(y_val)))), axis=0)\n",
        "y_test = np.concatenate((y_test_non_fraud, y[fraud_idx]), axis=0)\n",
        "\n",
        "# Define the variational autoencoder architecture\n",
        "input_dim = X_train.shape[1]\n",
        "latent_dim = 12  # Choose the dimension of the latent space\n",
        "\n",
        "# Encoder architecture\n",
        "encoder_input = Input(shape=(input_dim,))\n",
        "x = Dense(20, activation='relu')(encoder_input)  # Reduced size hidden layer\n",
        "x = Dense(12, activation='relu')(x)  # Additional hidden layer\n",
        "z_mean = Dense(latent_dim)(x)\n",
        "z_log_var = Dense(latent_dim)(x)\n",
        "\n",
        "# Sampling function\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(tf.keras.backend.shape(z_mean)[0], latent_dim), mean=0., stddev=1.0)\n",
        "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "# Decoder architecture\n",
        "decoder_input = Input(shape=(latent_dim,))\n",
        "x = Dense(12, activation='relu')(decoder_input)  # Additional hidden layer\n",
        "x = Dense(20, activation='relu')(x)  # Reduced size hidden layer\n",
        "decoder_output = Dense(input_dim, activation='sigmoid')(x)\n",
        "\n",
        "# Define the VAE model\n",
        "encoder = Model(encoder_input, [z_mean, z_log_var, z], name='encoder')\n",
        "decoder = Model(decoder_input, decoder_output, name='decoder')\n",
        "vae_output = decoder(encoder(encoder_input)[2])\n",
        "vae = Model(encoder_input, vae_output, name='vae')\n",
        "\n",
        "# Define the VAE loss\n",
        "reconstruction_loss = mse(encoder_input, vae_output)\n",
        "kl_loss = -0.5 * tf.keras.backend.mean(1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var), axis=-1)\n",
        "vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "\n",
        "# Compile the VAE model\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# Train the VAE\n",
        "vae.fit(X_train, X_train, epochs=100, batch_size=32, shuffle=True, validation_data=(X_val, X_val))\n",
        "\n",
        "# Compute reconstruction errors for training data\n",
        "train_reconstructions = vae.predict(X_train)\n",
        "reconstruction_errors_train = np.mean(np.square(X_train - train_reconstructions), axis=1)\n",
        "\n",
        "# Compute the mean reconstruction error on non-fraud training data\n",
        "non_fraud_train_errors = reconstruction_errors_train\n",
        "\n",
        "# Calculate threshold using the reconstruction errors of non-fraudulent transactions in the training set\n",
        "threshold = np.mean(non_fraud_train_errors) + 1.5 * np.std(non_fraud_train_errors)  # Adjust threshold as needed\n",
        "\n",
        "# Compute reconstruction errors for testing data\n",
        "test_reconstructions = vae.predict(X_test)\n",
        "reconstruction_errors_test = np.mean(np.square(X_test - test_reconstructions), axis=1)\n",
        "\n",
        "# Classify transactions as fraud or non-fraud based on the threshold\n",
        "y_pred_test = [1 if e > threshold else 0 for e in reconstruction_errors_test]\n"
      ],
      "metadata": {
        "id": "VQiNJv7WpdKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed3984c-8a0c-4ae7-9a7a-afc6d83a1698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8876/8876 [==============================] - 23s 2ms/step - loss: 0.0031 - val_loss: 0.0026\n",
            "Epoch 2/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 3/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 4/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 5/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 6/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 7/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 8/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 9/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 10/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 11/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 12/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 13/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 14/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 15/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 16/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 17/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 18/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 19/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 20/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 21/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 22/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 23/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 24/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 25/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 26/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 27/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 28/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 29/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 30/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 31/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 32/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 33/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 34/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 35/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 36/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 37/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 38/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 39/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 40/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 41/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 42/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 43/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 44/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 45/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 46/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 47/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 48/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 49/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 50/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 51/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 52/100\n",
            "8876/8876 [==============================] - 20s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 53/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 54/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 55/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 56/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 57/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 58/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 59/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 60/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 61/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 62/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 63/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 64/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 65/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 66/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 67/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 68/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 69/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 70/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 71/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 72/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 73/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 74/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 75/100\n",
            "8876/8876 [==============================] - 36s 4ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 76/100\n",
            "8876/8876 [==============================] - 32s 4ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 77/100\n",
            "8876/8876 [==============================] - 31s 4ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 78/100\n",
            "8876/8876 [==============================] - 34s 4ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 79/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 80/100\n",
            "8876/8876 [==============================] - 34s 4ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 81/100\n",
            "8876/8876 [==============================] - 33s 4ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 82/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 83/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 84/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 85/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 86/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 87/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 88/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 89/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 90/100\n",
            "8876/8876 [==============================] - 32s 4ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 91/100\n",
            "8876/8876 [==============================] - 31s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 92/100\n",
            "8876/8876 [==============================] - 31s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 93/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 94/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 95/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 96/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 97/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 98/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 99/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 100/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "8876/8876 [==============================] - 16s 2ms/step\n",
            "25/25 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate model performance on test set using second Variational Auto Encoder**"
      ],
      "metadata": {
        "id": "INLSNrMrgB2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model performance on test set\n",
        "from sklearn.metrics import precision_score,  recall_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred_test)\n",
        "print(f\"precision is: {precision}\")\n",
        "recall = recall_score(y_test, y_pred_test)\n",
        "print(f\"recall is: {recall}\")\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_test)\n",
        "print(f'F1 score on test set: {f1}')"
      ],
      "metadata": {
        "id": "2F49wSxK0JvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03479ca8-3cf7-4cee-820c-814fc82cd6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision is: 0.9691211401425178\n",
            "recall is: 0.8292682926829268\n",
            "F1 score on test set: 0.8937568455640744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SUMMARY**\n",
        "\n",
        "**Introduction**: This code implements a Variational Autoencoder (VAE) for anomaly detection in credit card transactions. The VAE is a deep generative model that learns to reconstruct the input data while also learning a compressed, latent representation of the data.\n",
        "\n",
        "**Data Preprocessing**: The code loads a credit card transaction dataset from a CSV file and preprocesses it. The target variable 'Class' (fraud or non-fraud) is separated from the features. The 'Time' and 'Amount' features are log-scaled, and all features are normalized to the range (0, 1) using MinMaxScaler. The dataset is then split into training, validation, and test sets, ensuring that only non-fraudulent transactions are included in the train set and ensuring that the test set is a combination of fraud and non-fraud transactions.\n",
        "\n",
        "**VAE Architecture**: The VAE consists of an encoder and a decoder network. The encoder maps the input data to a lower-dimensional latent space, capturing the essential features of the data. The decoder reconstructs the original data from the latent space representation. The architectures of the encoder and decoder are defined using dense layers with ReLU activations.\n",
        "\n",
        "**Model Training**: The VAE is trained on the non-fraudulent transactions in the training set. The loss function is a combination of the reconstruction loss (mean squared error) and the Kullback-Leibler (KL) divergence loss, which regularizes the latent space. The model is trained for 100 epochs, and the validation set is used to monitor the training process. We trained two auto encoders: First autoencoder: 1 encoder layer, 1 bottleneck layer, 1 decoder layer Second autoencoder: 2 encoder layers, 1 bottleneck layer, 2 decoder layers\n",
        "\n",
        "**Anomaly Detection**: After training, the VAE computes the reconstruction errors for the training and test sets. The reconstruction error is the mean squared difference between the original input and its reconstructed counterpart. A threshold is calculated based on the reconstruction errors of non-fraudulent transactions in the training set. Transactions with reconstruction errors above the threshold are classified as anomalies (potential fraud).\n",
        "\n",
        "**Model Evaluation**: The code evaluates the VAE's performance on the test set using the F1-score, precision, and recall metrics for the anomaly detection task.The code also includes a variation of the VAE architecture, where the number of hidden layers and their sizes are modified to explore the impact on model performance.\n",
        "\n",
        "**Performance metrics of First VAE:**\n",
        "\n",
        "precision is: 0.9691211401425178\n",
        "\n",
        "recall is: 0.8292682926829268\n",
        "\n",
        "F1 score on test set: 0.8937568455640744\n",
        "\n",
        "**Performance metrics of Second VAE:**\n",
        "\n",
        "precision is: 0.9691211401425178\n",
        "\n",
        "recall is: 0.8292682926829268\n",
        "\n",
        "F1 score on test set: 0.8937568455640744"
      ],
      "metadata": {
        "id": "Bs9ptfSxBzS0"
      }
    }
  ]
}