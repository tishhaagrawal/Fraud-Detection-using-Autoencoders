{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fraud Detection Using AutoEncoders**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bqhZvWUWdYrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the First Auto Encoder**"
      ],
      "metadata": {
        "id": "K-1osLErejYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load and preprocess data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/creditcard.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']\n",
        "\n",
        "# Log-scale the time difference and amount features\n",
        "time_diff_idx = X.columns.get_loc('Time')\n",
        "amount_idx = X.columns.get_loc('Amount')\n",
        "\n",
        "X.iloc[:, time_diff_idx] = np.log1p(X.iloc[:, time_diff_idx])  # Log-scale time difference\n",
        "X.iloc[:, amount_idx] = np.log1p(X.iloc[:, amount_idx])  # Log-scale amount\n",
        "\n",
        "# Normalize features to the range (0, 1)\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Separate fraud and non-fraud transactions\n",
        "non_fraud_idx = np.where(y == 0)[0]\n",
        "fraud_idx = np.where(y == 1)[0]\n",
        "\n",
        "# Split non-fraud data into train and validation sets\n",
        "X_non_fraud = X[non_fraud_idx]\n",
        "y_non_fraud = y[non_fraud_idx]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_non_fraud, y_non_fraud, test_size=0.001, random_state=42)\n",
        "\n",
        "# Create test set with remaining non-fraud data and all fraud data\n",
        "X_test_non_fraud = np.delete(X[non_fraud_idx], np.concatenate((np.arange(len(X_train)), np.arange(len(X_val)))), axis=0)\n",
        "X_test = np.concatenate((X_test_non_fraud, X[fraud_idx]), axis=0)\n",
        "y_test_non_fraud = np.delete(y[non_fraud_idx], np.concatenate((np.arange(len(y_train)), np.arange(len(y_val)))), axis=0)\n",
        "y_test = np.concatenate((y_test_non_fraud, y[fraud_idx]), axis=0)\n",
        "\n",
        "# Define the autoencoder architecture\n",
        "input_dim = X_train.shape[1]\n",
        "encoding_dim = 14\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoded = Dense(encoding_dim, activation='tanh')(input_layer)\n",
        "decoded = Dense(input_dim, activation='tanh')(encoded)\n",
        "\n",
        "# Create the autoencoder model\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(X_train, X_train, epochs=100, batch_size=32, shuffle=True, validation_data=(X_val, X_val))\n",
        "\n",
        "# Compute reconstruction errors for training data\n",
        "train_reconstructions = autoencoder.predict(X_train)\n",
        "reconstruction_errors_train = np.mean(np.square(X_train - train_reconstructions), axis=1)\n",
        "non_fraud_train_errors = reconstruction_errors_train\n",
        "\n",
        "# Calculate threshold using the reconstruction errors of non-fraudulent transactions in the training set\n",
        "threshold = np.mean(non_fraud_train_errors) + 2.5 * np.std(non_fraud_train_errors)\n",
        "\n",
        "# Compute reconstruction errors for testing data\n",
        "test_reconstructions = autoencoder.predict(X_test)\n",
        "reconstruction_errors_test = np.mean(np.square(X_test - test_reconstructions), axis=1)\n",
        "\n",
        "# Classify transactions as fraud or non-fraud based on the threshold\n",
        "y_pred_test = [1 if e > threshold else 0 for e in reconstruction_errors_test]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbpXn6pqmmne",
        "outputId": "ee3a9e8a-9593-4e00-91c2-20cf27e94578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 0.0025 - val_loss: 4.1112e-04\n",
            "Epoch 2/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 2.9977e-04 - val_loss: 2.1401e-04\n",
            "Epoch 3/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 2.2685e-04 - val_loss: 1.9409e-04\n",
            "Epoch 4/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 2.1356e-04 - val_loss: 1.8384e-04\n",
            "Epoch 5/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 2.0335e-04 - val_loss: 1.7507e-04\n",
            "Epoch 6/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.9536e-04 - val_loss: 1.6612e-04\n",
            "Epoch 7/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.8967e-04 - val_loss: 1.6448e-04\n",
            "Epoch 8/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 1.8584e-04 - val_loss: 1.6106e-04\n",
            "Epoch 9/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.8266e-04 - val_loss: 1.5855e-04\n",
            "Epoch 10/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.7980e-04 - val_loss: 1.5381e-04\n",
            "Epoch 11/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.7657e-04 - val_loss: 1.5211e-04\n",
            "Epoch 12/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.7337e-04 - val_loss: 1.4959e-04\n",
            "Epoch 13/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.7076e-04 - val_loss: 1.4639e-04\n",
            "Epoch 14/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.6885e-04 - val_loss: 1.4991e-04\n",
            "Epoch 15/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 1.6743e-04 - val_loss: 1.4183e-04\n",
            "Epoch 16/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 1.6647e-04 - val_loss: 1.4718e-04\n",
            "Epoch 17/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 1.6563e-04 - val_loss: 1.4008e-04\n",
            "Epoch 18/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.6494e-04 - val_loss: 1.3928e-04\n",
            "Epoch 19/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.6423e-04 - val_loss: 1.3995e-04\n",
            "Epoch 20/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 1.6366e-04 - val_loss: 1.3774e-04\n",
            "Epoch 21/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.6306e-04 - val_loss: 1.4033e-04\n",
            "Epoch 22/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.6227e-04 - val_loss: 1.4208e-04\n",
            "Epoch 23/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 1.6173e-04 - val_loss: 1.3723e-04\n",
            "Epoch 24/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.6108e-04 - val_loss: 1.3522e-04\n",
            "Epoch 25/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.6097e-04 - val_loss: 1.5590e-04\n",
            "Epoch 26/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 1.6066e-04 - val_loss: 1.3546e-04\n",
            "Epoch 27/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.6018e-04 - val_loss: 1.3677e-04\n",
            "Epoch 28/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.6025e-04 - val_loss: 1.3587e-04\n",
            "Epoch 29/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.6015e-04 - val_loss: 1.4301e-04\n",
            "Epoch 30/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.6007e-04 - val_loss: 1.3555e-04\n",
            "Epoch 31/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.5995e-04 - val_loss: 1.3539e-04\n",
            "Epoch 32/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5971e-04 - val_loss: 1.3831e-04\n",
            "Epoch 33/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5972e-04 - val_loss: 1.3533e-04\n",
            "Epoch 34/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 1.5969e-04 - val_loss: 1.3408e-04\n",
            "Epoch 35/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5956e-04 - val_loss: 1.3486e-04\n",
            "Epoch 36/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5953e-04 - val_loss: 1.3522e-04\n",
            "Epoch 37/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.5960e-04 - val_loss: 1.3403e-04\n",
            "Epoch 38/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 1.5928e-04 - val_loss: 1.3557e-04\n",
            "Epoch 39/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5928e-04 - val_loss: 1.3397e-04\n",
            "Epoch 40/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 1.5921e-04 - val_loss: 1.3476e-04\n",
            "Epoch 41/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 1.5924e-04 - val_loss: 1.3469e-04\n",
            "Epoch 42/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5917e-04 - val_loss: 1.3467e-04\n",
            "Epoch 43/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 1.5908e-04 - val_loss: 1.3493e-04\n",
            "Epoch 44/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 1.5881e-04 - val_loss: 1.3415e-04\n",
            "Epoch 45/100\n",
            "8876/8876 [==============================] - 21s 2ms/step - loss: 1.5899e-04 - val_loss: 1.3372e-04\n",
            "Epoch 46/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5893e-04 - val_loss: 1.3935e-04\n",
            "Epoch 47/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5872e-04 - val_loss: 1.3387e-04\n",
            "Epoch 48/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.5884e-04 - val_loss: 1.3348e-04\n",
            "Epoch 49/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5869e-04 - val_loss: 1.3795e-04\n",
            "Epoch 50/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5854e-04 - val_loss: 1.3437e-04\n",
            "Epoch 51/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5867e-04 - val_loss: 1.3454e-04\n",
            "Epoch 52/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5868e-04 - val_loss: 1.3596e-04\n",
            "Epoch 53/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 1.5838e-04 - val_loss: 1.3260e-04\n",
            "Epoch 54/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.5846e-04 - val_loss: 1.3271e-04\n",
            "Epoch 55/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5833e-04 - val_loss: 1.3699e-04\n",
            "Epoch 56/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5839e-04 - val_loss: 1.3320e-04\n",
            "Epoch 57/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5826e-04 - val_loss: 1.3869e-04\n",
            "Epoch 58/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5837e-04 - val_loss: 1.3392e-04\n",
            "Epoch 59/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5810e-04 - val_loss: 1.3413e-04\n",
            "Epoch 60/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.5828e-04 - val_loss: 1.3530e-04\n",
            "Epoch 61/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5800e-04 - val_loss: 1.3511e-04\n",
            "Epoch 62/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 1.5799e-04 - val_loss: 1.3341e-04\n",
            "Epoch 63/100\n",
            "8876/8876 [==============================] - 22s 2ms/step - loss: 1.5796e-04 - val_loss: 1.3393e-04\n",
            "Epoch 64/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5801e-04 - val_loss: 1.3296e-04\n",
            "Epoch 65/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5774e-04 - val_loss: 1.3287e-04\n",
            "Epoch 66/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5772e-04 - val_loss: 1.3348e-04\n",
            "Epoch 67/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5777e-04 - val_loss: 1.3513e-04\n",
            "Epoch 68/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5760e-04 - val_loss: 1.3629e-04\n",
            "Epoch 69/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5772e-04 - val_loss: 1.3406e-04\n",
            "Epoch 70/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5771e-04 - val_loss: 1.3350e-04\n",
            "Epoch 71/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5754e-04 - val_loss: 1.3332e-04\n",
            "Epoch 72/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 1.5752e-04 - val_loss: 1.3356e-04\n",
            "Epoch 73/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5749e-04 - val_loss: 1.3460e-04\n",
            "Epoch 74/100\n",
            "8876/8876 [==============================] - 22s 3ms/step - loss: 1.5745e-04 - val_loss: 1.3264e-04\n",
            "Epoch 75/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 1.5733e-04 - val_loss: 1.3549e-04\n",
            "Epoch 76/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 1.5726e-04 - val_loss: 1.3687e-04\n",
            "Epoch 77/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 1.5728e-04 - val_loss: 1.3250e-04\n",
            "Epoch 78/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 1.5716e-04 - val_loss: 1.3629e-04\n",
            "Epoch 79/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.5715e-04 - val_loss: 1.3232e-04\n",
            "Epoch 80/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 1.5718e-04 - val_loss: 1.3280e-04\n",
            "Epoch 81/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5717e-04 - val_loss: 1.3386e-04\n",
            "Epoch 82/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5695e-04 - val_loss: 1.3211e-04\n",
            "Epoch 83/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 1.5696e-04 - val_loss: 1.3240e-04\n",
            "Epoch 84/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 1.5701e-04 - val_loss: 1.8065e-04\n",
            "Epoch 85/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5695e-04 - val_loss: 1.3311e-04\n",
            "Epoch 86/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5693e-04 - val_loss: 1.3697e-04\n",
            "Epoch 87/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5675e-04 - val_loss: 1.3352e-04\n",
            "Epoch 88/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5698e-04 - val_loss: 1.3259e-04\n",
            "Epoch 89/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 1.5676e-04 - val_loss: 1.3263e-04\n",
            "Epoch 90/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 1.5670e-04 - val_loss: 1.3445e-04\n",
            "Epoch 91/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 1.5673e-04 - val_loss: 1.3260e-04\n",
            "Epoch 92/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5651e-04 - val_loss: 1.3408e-04\n",
            "Epoch 93/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 1.5667e-04 - val_loss: 1.3193e-04\n",
            "Epoch 94/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5669e-04 - val_loss: 1.3273e-04\n",
            "Epoch 95/100\n",
            "8876/8876 [==============================] - 24s 3ms/step - loss: 1.5653e-04 - val_loss: 1.3717e-04\n",
            "Epoch 96/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5662e-04 - val_loss: 1.3333e-04\n",
            "Epoch 97/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 1.5653e-04 - val_loss: 1.3267e-04\n",
            "Epoch 98/100\n",
            "8876/8876 [==============================] - 23s 3ms/step - loss: 1.5649e-04 - val_loss: 1.3348e-04\n",
            "Epoch 99/100\n",
            "8876/8876 [==============================] - 25s 3ms/step - loss: 1.5649e-04 - val_loss: 1.3205e-04\n",
            "Epoch 100/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 1.5648e-04 - val_loss: 1.3254e-04\n",
            "8876/8876 [==============================] - 20s 2ms/step\n",
            "25/25 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate model performance on test set using first Auto Encoder**"
      ],
      "metadata": {
        "id": "92LwILL5dzxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score,  recall_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred_test)\n",
        "print(f\"precision is: {precision}\")\n",
        "recall = recall_score(y_test, y_pred_test)\n",
        "print(f\"recall is: {recall}\")\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_test)\n",
        "print(f'F1 score on test set: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsHOfviNzqUV",
        "outputId": "20698289-ceb9-4255-94f9-30395947aa4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision is: 0.9898477157360406\n",
            "recall is: 0.7926829268292683\n",
            "F1 score on test set: 0.8803611738148984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting Reconstruction Error Distribution**"
      ],
      "metadata": {
        "id": "l6sZtEX4d-jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(non_fraud_train_errors, bins=50, alpha=0.5, label='Non-Fraud', density=True)\n",
        "plt.axvline(threshold, color='r', linestyle='--', label='Threshold')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Reconstruction Error Distribution')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "bdjzvbp90FWs",
        "outputId": "e202c862-62ad-4e96-e961-cdf71e99e931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the Second Auto Encoder**"
      ],
      "metadata": {
        "id": "aTxXVotEeJbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load and preprocess data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/creditcard.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']\n",
        "\n",
        "# Log-scale the time difference and amount features\n",
        "time_diff_idx = X.columns.get_loc('Time')\n",
        "amount_idx = X.columns.get_loc('Amount')\n",
        "\n",
        "X.iloc[:, time_diff_idx] = np.log1p(X.iloc[:, time_diff_idx])  # Log-scale time difference\n",
        "X.iloc[:, amount_idx] = np.log1p(X.iloc[:, amount_idx])  # Log-scale amount\n",
        "\n",
        "# Normalize features to the range (0, 1)\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Separate fraud and non-fraud transactions\n",
        "non_fraud_idx = np.where(y == 0)[0]\n",
        "fraud_idx = np.where(y == 1)[0]\n",
        "\n",
        "# Split non-fraud data into train and validation sets\n",
        "X_non_fraud = X[non_fraud_idx]\n",
        "y_non_fraud = y[non_fraud_idx]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_non_fraud, y_non_fraud, test_size=0.001, random_state=42)\n",
        "\n",
        "# Create test set with remaining non-fraud data and all fraud data\n",
        "X_test_non_fraud = np.delete(X[non_fraud_idx], np.concatenate((np.arange(len(X_train)), np.arange(len(X_val)))), axis=0)\n",
        "X_test = np.concatenate((X_test_non_fraud, X[fraud_idx]), axis=0)\n",
        "y_test_non_fraud = np.delete(y[non_fraud_idx], np.concatenate((np.arange(len(y_train)), np.arange(len(y_val)))), axis=0)\n",
        "y_test = np.concatenate((y_test_non_fraud, y[fraud_idx]), axis=0)\n",
        "\n",
        "# Define the autoencoder architecture\n",
        "input_dim = X_train.shape[1]\n",
        "encoding_dim = 10  # Choose the dimension of the encoded representation\n",
        "hidden_dim = 14  # Dimension of the hidden layers\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "hidden_encoder = Dense(hidden_dim, activation='tanh')(input_layer)\n",
        "encoded = Dense(encoding_dim, activation='tanh')(hidden_encoder)\n",
        "hidden_decoder = Dense(hidden_dim, activation='tanh')(encoded)\n",
        "decoded = Dense(input_dim, activation='tanh')(hidden_decoder)\n",
        "\n",
        "# Create the autoencoder model\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(X_train, X_train, epochs=100, batch_size=32, shuffle=True, validation_data=(X_val, X_val))\n",
        "\n",
        "# Compute reconstruction errors for training data\n",
        "train_reconstructions = autoencoder.predict(X_train)\n",
        "reconstruction_errors_train = np.mean(np.square(X_train - train_reconstructions), axis=1)\n",
        "\n",
        "# Compute the mean reconstruction error on non-fraud training data\n",
        "non_fraud_train_errors = reconstruction_errors_train\n",
        "\n",
        "# Calculate threshold using the reconstruction errors of non-fraudulent transactions in the training set\n",
        "threshold = np.mean(non_fraud_train_errors) + 2.5 * np.std(non_fraud_train_errors)\n",
        "\n",
        "# Compute reconstruction errors for testing data\n",
        "test_reconstructions = autoencoder.predict(X_test)\n",
        "reconstruction_errors_test = np.mean(np.square(X_test - test_reconstructions), axis=1)\n",
        "\n",
        "# Classify transactions as fraud or non-fraud based on the threshold\n",
        "y_pred_test = [1 if e > threshold else 0 for e in reconstruction_errors_test]\n",
        "\n",
        "# Evaluate model performance on test set\n",
        "f1 = f1_score(y_test, y_pred_test)\n",
        "print(f'F1 score on test set: {f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDNDwWVQvE1m",
        "outputId": "ac43e58e-7740-4fb0-9c55-0b725831f6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8876/8876 [==============================] - 33s 4ms/step - loss: 0.0024 - val_loss: 5.2770e-04\n",
            "Epoch 2/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 4.2442e-04 - val_loss: 3.3685e-04\n",
            "Epoch 3/100\n",
            "8876/8876 [==============================] - 32s 4ms/step - loss: 3.4157e-04 - val_loss: 3.0721e-04\n",
            "Epoch 4/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 3.3235e-04 - val_loss: 3.0600e-04\n",
            "Epoch 5/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 3.2660e-04 - val_loss: 3.0566e-04\n",
            "Epoch 6/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 3.2296e-04 - val_loss: 3.0640e-04\n",
            "Epoch 7/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 3.1769e-04 - val_loss: 2.9928e-04\n",
            "Epoch 8/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 3.1002e-04 - val_loss: 2.9737e-04\n",
            "Epoch 9/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 3.0325e-04 - val_loss: 2.7274e-04\n",
            "Epoch 10/100\n",
            "8876/8876 [==============================] - 34s 4ms/step - loss: 2.9761e-04 - val_loss: 2.6926e-04\n",
            "Epoch 11/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.9439e-04 - val_loss: 2.6693e-04\n",
            "Epoch 12/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.9162e-04 - val_loss: 2.6084e-04\n",
            "Epoch 13/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.8931e-04 - val_loss: 2.6265e-04\n",
            "Epoch 14/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.8735e-04 - val_loss: 2.5751e-04\n",
            "Epoch 15/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.8557e-04 - val_loss: 2.5481e-04\n",
            "Epoch 16/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.8441e-04 - val_loss: 2.5557e-04\n",
            "Epoch 17/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.8346e-04 - val_loss: 2.5124e-04\n",
            "Epoch 18/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.8272e-04 - val_loss: 2.5263e-04\n",
            "Epoch 19/100\n",
            "8876/8876 [==============================] - 31s 3ms/step - loss: 2.8184e-04 - val_loss: 2.5223e-04\n",
            "Epoch 20/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.7939e-04 - val_loss: 2.4996e-04\n",
            "Epoch 21/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.7509e-04 - val_loss: 2.4369e-04\n",
            "Epoch 22/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.7200e-04 - val_loss: 2.4413e-04\n",
            "Epoch 23/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.7056e-04 - val_loss: 2.4189e-04\n",
            "Epoch 24/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.6942e-04 - val_loss: 2.3918e-04\n",
            "Epoch 25/100\n",
            "8876/8876 [==============================] - 32s 4ms/step - loss: 2.6830e-04 - val_loss: 2.3584e-04\n",
            "Epoch 26/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.6754e-04 - val_loss: 2.3604e-04\n",
            "Epoch 27/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.6676e-04 - val_loss: 2.3489e-04\n",
            "Epoch 28/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.6600e-04 - val_loss: 2.3363e-04\n",
            "Epoch 29/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 2.6527e-04 - val_loss: 2.3941e-04\n",
            "Epoch 30/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.6470e-04 - val_loss: 2.3450e-04\n",
            "Epoch 31/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.6413e-04 - val_loss: 2.3287e-04\n",
            "Epoch 32/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.6356e-04 - val_loss: 2.3154e-04\n",
            "Epoch 33/100\n",
            "8876/8876 [==============================] - 34s 4ms/step - loss: 2.6312e-04 - val_loss: 2.3009e-04\n",
            "Epoch 34/100\n",
            "8876/8876 [==============================] - 32s 4ms/step - loss: 2.6256e-04 - val_loss: 2.3018e-04\n",
            "Epoch 35/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.6205e-04 - val_loss: 2.2937e-04\n",
            "Epoch 36/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.6153e-04 - val_loss: 2.2844e-04\n",
            "Epoch 37/100\n",
            "8876/8876 [==============================] - 31s 3ms/step - loss: 2.6115e-04 - val_loss: 2.3515e-04\n",
            "Epoch 38/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.6065e-04 - val_loss: 2.2690e-04\n",
            "Epoch 39/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.6026e-04 - val_loss: 2.3059e-04\n",
            "Epoch 40/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.5973e-04 - val_loss: 2.2852e-04\n",
            "Epoch 41/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5946e-04 - val_loss: 2.3056e-04\n",
            "Epoch 42/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.5904e-04 - val_loss: 2.2642e-04\n",
            "Epoch 43/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.5855e-04 - val_loss: 2.2653e-04\n",
            "Epoch 44/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.5812e-04 - val_loss: 2.2460e-04\n",
            "Epoch 45/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5776e-04 - val_loss: 2.2568e-04\n",
            "Epoch 46/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.5756e-04 - val_loss: 2.2343e-04\n",
            "Epoch 47/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5716e-04 - val_loss: 2.2817e-04\n",
            "Epoch 48/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.5687e-04 - val_loss: 2.2780e-04\n",
            "Epoch 49/100\n",
            "8876/8876 [==============================] - 26s 3ms/step - loss: 2.5659e-04 - val_loss: 2.2329e-04\n",
            "Epoch 50/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5641e-04 - val_loss: 2.2720e-04\n",
            "Epoch 51/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5604e-04 - val_loss: 2.2640e-04\n",
            "Epoch 52/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5578e-04 - val_loss: 2.2435e-04\n",
            "Epoch 53/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5555e-04 - val_loss: 2.3104e-04\n",
            "Epoch 54/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5552e-04 - val_loss: 2.2781e-04\n",
            "Epoch 55/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 2.5526e-04 - val_loss: 2.2636e-04\n",
            "Epoch 56/100\n",
            "8876/8876 [==============================] - 32s 4ms/step - loss: 2.5502e-04 - val_loss: 2.2356e-04\n",
            "Epoch 57/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 2.5466e-04 - val_loss: 2.2350e-04\n",
            "Epoch 58/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.5438e-04 - val_loss: 2.2699e-04\n",
            "Epoch 59/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.5442e-04 - val_loss: 2.2158e-04\n",
            "Epoch 60/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5415e-04 - val_loss: 2.2158e-04\n",
            "Epoch 61/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.5391e-04 - val_loss: 2.2123e-04\n",
            "Epoch 62/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5377e-04 - val_loss: 2.2086e-04\n",
            "Epoch 63/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.5350e-04 - val_loss: 2.2225e-04\n",
            "Epoch 64/100\n",
            "8876/8876 [==============================] - 31s 3ms/step - loss: 2.5353e-04 - val_loss: 2.2156e-04\n",
            "Epoch 65/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.5322e-04 - val_loss: 2.2587e-04\n",
            "Epoch 66/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5303e-04 - val_loss: 2.3064e-04\n",
            "Epoch 67/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.5289e-04 - val_loss: 2.1988e-04\n",
            "Epoch 68/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5260e-04 - val_loss: 2.2222e-04\n",
            "Epoch 69/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5255e-04 - val_loss: 2.1901e-04\n",
            "Epoch 70/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.5240e-04 - val_loss: 2.2273e-04\n",
            "Epoch 71/100\n",
            "8876/8876 [==============================] - 31s 3ms/step - loss: 2.5232e-04 - val_loss: 2.1967e-04\n",
            "Epoch 72/100\n",
            "8876/8876 [==============================] - 31s 3ms/step - loss: 2.5202e-04 - val_loss: 2.2520e-04\n",
            "Epoch 73/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5194e-04 - val_loss: 2.2471e-04\n",
            "Epoch 74/100\n",
            "8876/8876 [==============================] - 31s 3ms/step - loss: 2.5155e-04 - val_loss: 2.2207e-04\n",
            "Epoch 75/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.5138e-04 - val_loss: 2.2031e-04\n",
            "Epoch 76/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.5122e-04 - val_loss: 2.2506e-04\n",
            "Epoch 77/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5104e-04 - val_loss: 2.1977e-04\n",
            "Epoch 78/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.5080e-04 - val_loss: 2.1975e-04\n",
            "Epoch 79/100\n",
            "8876/8876 [==============================] - 31s 4ms/step - loss: 2.5055e-04 - val_loss: 2.2366e-04\n",
            "Epoch 80/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.5038e-04 - val_loss: 2.1856e-04\n",
            "Epoch 81/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.5012e-04 - val_loss: 2.1669e-04\n",
            "Epoch 82/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.4986e-04 - val_loss: 2.2608e-04\n",
            "Epoch 83/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.4960e-04 - val_loss: 2.2007e-04\n",
            "Epoch 84/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.4956e-04 - val_loss: 2.2068e-04\n",
            "Epoch 85/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.4935e-04 - val_loss: 2.2053e-04\n",
            "Epoch 86/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.4924e-04 - val_loss: 2.1905e-04\n",
            "Epoch 87/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.4911e-04 - val_loss: 2.1582e-04\n",
            "Epoch 88/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.4898e-04 - val_loss: 2.2106e-04\n",
            "Epoch 89/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.4881e-04 - val_loss: 2.1791e-04\n",
            "Epoch 90/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.4867e-04 - val_loss: 2.1642e-04\n",
            "Epoch 91/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.4860e-04 - val_loss: 2.2125e-04\n",
            "Epoch 92/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.4846e-04 - val_loss: 2.1859e-04\n",
            "Epoch 93/100\n",
            "8876/8876 [==============================] - 30s 3ms/step - loss: 2.4853e-04 - val_loss: 2.1514e-04\n",
            "Epoch 94/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.4829e-04 - val_loss: 2.1500e-04\n",
            "Epoch 95/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.4812e-04 - val_loss: 2.2226e-04\n",
            "Epoch 96/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.4807e-04 - val_loss: 2.1795e-04\n",
            "Epoch 97/100\n",
            "8876/8876 [==============================] - 28s 3ms/step - loss: 2.4801e-04 - val_loss: 2.1905e-04\n",
            "Epoch 98/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.4786e-04 - val_loss: 2.1523e-04\n",
            "Epoch 99/100\n",
            "8876/8876 [==============================] - 27s 3ms/step - loss: 2.4779e-04 - val_loss: 2.1595e-04\n",
            "Epoch 100/100\n",
            "8876/8876 [==============================] - 29s 3ms/step - loss: 2.4759e-04 - val_loss: 2.1806e-04\n",
            "8876/8876 [==============================] - 19s 2ms/step\n",
            "25/25 [==============================] - 0s 2ms/step\n",
            "F1 score on test set: 0.9036544850498338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate model performance on test set using second Auto Encoder**"
      ],
      "metadata": {
        "id": "JUAmgYeuexVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score,  recall_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred_test)\n",
        "print(f\"precision is: {precision}\")\n",
        "recall = recall_score(y_test, y_pred_test)\n",
        "print(f\"recall is: {recall}\")\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_test)\n",
        "print(f'F1 score on test set: {f1}')"
      ],
      "metadata": {
        "id": "4VlR1F9oVh8O",
        "outputId": "c2aa9401-8fc5-405d-bf42-117c7e2314bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision is: 0.9927007299270073\n",
            "recall is: 0.8292682926829268\n",
            "F1 score on test set: 0.9036544850498338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SUMMARY**\n",
        "\n",
        "**Introduction**: This code implements an autoencoder model for fraud detection in credit card transactions. The autoencoder is an unsupervised learning technique that learns to reconstruct the input data by compressing it into a lower-dimensional representation and then decompressing it back to the original dimensions.\n",
        "\n",
        "**Data Preprocessing**: The code loads a credit card transaction dataset from a CSV file and preprocesses it. The target variable 'Class' (fraud or non-fraud) is separated from the features. The 'Time' and 'Amount' features are log-scaled, and all features are normalized to the range (0, 1) using MinMaxScaler. The dataset is then split into training, validation, and test sets, ensuring that only non-fraudulent transactions are included in the train set and ensuring that the test set is a combination of fraud and non-fraud transactions.\n",
        "\n",
        "**Autoencoder Architecture**: The autoencoder architecture consists of an input layer, a hidden encoder layer, a bottleneck layer (encoded representation), a hidden decoder layer, and an output layer.\n",
        "\n",
        "**Model Training**: The autoencoder is trained on the non-fraudulent transactions in the training set using the mean squared error loss function and the Adam optimizer. The model is trained for 100 epochs, and the validation set is used to monitor the training process. We trained two auto encoders:\n",
        "**First autoencoder:** 1 encoder layer, 1 bottleneck layer, 1 decoder layer\n",
        "**Second autoencoder:** 2 encoder layers, 1 bottleneck layer, 2 decoder layers\n",
        "\n",
        "\n",
        "**Fraud Detection**: After training, the autoencoder computes the reconstruction errors for the training and test sets. The reconstruction error is the mean squared difference between the original input and its reconstructed counterpart. A threshold is calculated based on the reconstruction errors of non-fraudulent transactions in the training set. Transactions with reconstruction errors above the threshold are classified as fraudulent.\n",
        "\n",
        "**Model Evaluation**: The code evaluates the autoencoder's performance on the test set using the F1-score, precision, and recall metrics. It also plots a histogram of the reconstruction error distribution for non-fraudulent transactions in the training set, highlighting the chosen threshold.\n",
        "\n",
        "**Performance metrics of First Auto Encoder:**\n",
        "\n",
        "precision is: 0.9898477157360406\n",
        "\n",
        "recall is: 0.7926829268292683\n",
        "\n",
        "F1 score on test set: 0.8803611738148984\n",
        "\n",
        "**Performance metrics of Second Auto Encoder:**\n",
        "\n",
        "precision is: 0.9927007299270073\n",
        "\n",
        "recall is: 0.8292682926829268\n",
        "\n",
        "F1 score on test set: 0.9036544850498338\n",
        "\n"
      ],
      "metadata": {
        "id": "fDBHOrGa9jQM"
      }
    }
  ]
}
